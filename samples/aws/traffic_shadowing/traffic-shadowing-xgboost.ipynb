{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring Amazon Sagemaker Endpoint with Hydrosphere\n",
    "\n",
    "This notebook shows how to:\n",
    "* Host a machine learning model in Amazon SageMaker and capture inference requests, results, and metadata\n",
    "* Deploy a lambda function to shadow traffic from Sagemaker endpoint to Hydrosphere platform\n",
    "* Analyze inference metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "Hydrosphere supports seamless integration with Amazon Sagemaker endpoints. You can setup a shadowing channel between your Sagemaker production endopint and a Hydrosphere instance to traverse all request/response pairs for more verbose traffic analysis by Hydrosphere.\n",
    "\n",
    "Amazon SageMaker provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly. Amazon SageMaker is a fully-managed service that encompasses the entire machine learning workflow. You can label and prepare your data, choose an algorithm, train a model, and then tune and optimize it for deployment. You can deploy your models to production with Amazon SageMaker to make predictions and lower costs than was previously possible.\n",
    "\n",
    "In addition, Amazon SageMaker enables you to capture the input, output and metadata for invocations of the models that you deploy. This ability lets you seemlessly integrate with Hydrosphere. This notebook shows you the details of setting up such a shadowing channel.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "To get started, make sure you have these prerequisites completed.\n",
    "\n",
    "* Specify an AWS Region to host your model.\n",
    "* An IAM role ARN exists that is used to give Amazon SageMaker access to your data in Amazon Simple Storage Service (Amazon S3). See the documentation for how to fine tune the permissions needed. **Important**, this role also has to perform operations with Amazon CloudFormation service and manage Amazon Lambda functions. See [hydro-integrations](https://github.com/Hydrospheredata/hydro-serving-integrations#before-you-start) repository for more details.\n",
    "* Create an S3 bucket used to store the data used to train your model, any additional model data, and the data captured from model invocations. For demonstration purposes, you are using the same bucket for these. In reality, you might want to separate them with different security policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Handful of configuration\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "from sagemaker import get_execution_role, session\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "print(\"RoleArn: {}\".format(role))\n",
    "\n",
    "# You can use a different bucket, but make sure the role you chose for this notebook\n",
    "# has the s3:PutObject permissions. This is the bucket into which the data is captured\n",
    "bucket =  session.Session(boto3.Session()).default_bucket()\n",
    "print(\"Demo Bucket: {}\".format(bucket))\n",
    "prefix = 'sagemaker/DEMO-ModelMonitor'\n",
    "\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "data_training_prefix = '{}/datatraining'.format(prefix)\n",
    "s3_capture_upload_path = 's3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "s3_training_upload_path = 's3://{}/{}'.format(bucket, data_training_prefix)\n",
    "print(\"Capture path: {}\".format(s3_capture_upload_path))\n",
    "print(\"Training path: {}\".format(s3_training_upload_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can quickly verify that the execution role for this notebook has the necessary permissions to proceed. Put a simple test object into the S3 bucket you speciÔ¨Åed above. If this command fails, update the role to have `s3:PutObject` permission on the bucket and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload some test files\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(\"test_upload/test.txt\").upload_file('test_data/upload-test-file.txt')\n",
    "print(\"Success! You are all set to proceed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing real-time inference data from Amazon SageMaker endpoints and shadowing it to Hydrosphere platform\n",
    "Create an endpoint to showcase the data capture capability in action.\n",
    "\n",
    "### Upload the pre-trained model to Amazon S3\n",
    "This code uploads a pre-trained XGBoost model that is ready for you to deploy. This model was trained using the XGB Churn Prediction Notebook in SageMaker. You can also use your own pre-trained model in this step. If you already have a pretrained model in Amazon S3, you can add it instead by specifying the s3_key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(\"model/xgb-churn-prediction-model.tar.gz\", 'rb')\n",
    "s3_key = os.path.join(prefix, 'xgb-churn-prediction-model.tar.gz')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_key).upload_fileobj(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model to Amazon SageMaker\n",
    "Start with deploying a pre-trained churn prediction model. Here, you create the model object with the image and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "model_name = \"DEMO-xgb-churn-pred-model-monitor-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = 'https://{}.s3-{}.amazonaws.com/{}/xgb-churn-prediction-model.tar.gz'.format(bucket, region, prefix)\n",
    "image_uri = get_image_uri(boto3.Session().region_name, 'xgboost', '0.90-1')\n",
    "\n",
    "model = Model(image=image_uri, model_data=model_url, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Go ahead with the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "endpoint_name = 'DEMO-xgb-churn-pred-model-monitor-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"EndpointName={}\".format(endpoint_name))\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture=True,\n",
    "                        sampling_percentage=100,\n",
    "                        destination_s3_uri=s3_capture_upload_path)\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                instance_type='ml.m5.large',\n",
    "                endpoint_name=endpoint_name,\n",
    "                data_capture_config=data_capture_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup traffic shadowing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, install `hydro-integrations` SDK, which will allow you to setup shadowing channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hydro-integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For monitoring purposes Hydrosphere requires an access to the training data for target model. In this example we place training data close to the `data_capture_prefix` prefix, but you can setup a different S3 URI if required. The minor constraint is to have training data placed under to format it in such a way, that will prefix endpoint name before \n",
    "\n",
    "Note, by default `destination_s3_uri` parameter, specified in the `data_capture_config`, represents a prefix where your requests will be stored. The minor constraint is to have training data placed under endpoint name, which you are monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydro_integrations.aws.sagemaker import TrafficShadowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = \"test_data/training-dataset-with-header.csv\"\n",
    "\n",
    "# Note, we are including endpoint_name to training path\n",
    "training_data_key = os.path.join(data_training_prefix, endpoint_name, \"training-dataset-with-header.csv\")\n",
    "\n",
    "boto3.Session().resource('s3').Object(bucket, training_data_key).upload_file(training_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy traffic shadowing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint has been provisioned and training data has been uploaded, we can deploy a lambda function, which will shadow traffic to Hydrosphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydrosphere_endpoint = \"<hydrosphere>\" # Update with your Hydrosphere instance endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadowing = TrafficShadowing(\n",
    "    hydrosphere_endpoint,\n",
    "    s3_training_upload_path,\n",
    "    data_capture_config,\n",
    ")\n",
    "\n",
    "shadowing.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open AWS Management Console and go to CloudFormation service. You will see a new stack `traffic-shadowing-hydrosphere-...` being provisioned. This stack spins up a Lambda function, which will listen to events under `s3_capture_upload_path` and notify Hydrosphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the deployed model\n",
    "\n",
    "You can now send data to this endpoint to get inferences in real time. Because you enabled the data capture in the previous steps, the request and response payload, along with some additional metadata, is saved in the Amazon Simple Storage Service (Amazon S3) location you have specified in the DataCaptureConfig."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step invokes the endpoint with included sample data for about 2 minutes. Data is captured based on the sampling percentage specified and the capture continues until the data capture option is turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "import time\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name,content_type='text/csv')\n",
    "\n",
    "# get a subset of test data for a quick test\n",
    "!head -120 test_data/test-dataset-input-cols.csv > test_data/test_sample.csv\n",
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait...\".format(endpoint_name))\n",
    "\n",
    "with open('test_data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(\"Done!\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Hydrosphere dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open Hydrosphere endpoint and navigate to the model with the same name as your Sagemaker endpoint name. Within this model collection you will see a model version with ID 1 (which is latest for this model), where you can find all details about that external model. You can open monitoring dashboard and observe captured requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadowing.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}